#!/usr/bin/env python3
"""Simple H100 compatibility test."""

import torch
from utils.hardware import HardwareDetector
from utils.mixed_precision_manager import MixedPrecisionManager

def test_h100_compatibility():
    print("=" * 50)
    print("H100 COMPATIBILITY TEST")
    print("=" * 50)
    
    # Test hardware detection
    print("\n1. Hardware Detection:")
    detector = HardwareDetector()
    config = detector.detect_gpu_config()
    print(f"   GPU: {config.get('gpu_name', 'Unknown')}")
    print(f"   Memory: {config.get('gpu_memory_gb', 0)} GB")
    print(f"   Mixed Precision: {config.get('mixed_precision_dtype', 'Unknown')}")
    print(f"   Batch Size: {config.get('batch_size', 1)}")
    
    # Test mixed precision manager for H100
    print("\n2. H100 Mixed Precision Config:")
    mp_h100 = MixedPrecisionManager(gpu_type="h100")
    h100_config = mp_h100.get_precision_config()
    print("   H100 Configuration:")
    for key, value in h100_config.items():
        print(f"     {key}: {value}")
    
    # Test current GPU mixed precision
    print("\n3. Current GPU Mixed Precision Config:")
    mp_auto = MixedPrecisionManager(gpu_type="auto")
    auto_config = mp_auto.get_precision_config()
    print(f"   Detected GPU Type: {mp_auto.gpu_type}")
    print("   Current GPU Configuration:")
    for key, value in auto_config.items():
        print(f"     {key}: {value}")
    
    # Test CUDA info
    print("\n4. CUDA Information:")
    if torch.cuda.is_available():
        print(f"   CUDA Available: Yes")
        print(f"   CUDA Version: {torch.version.cuda}")
        print(f"   Device Count: {torch.cuda.device_count()}")
        print(f"   Current Device: {torch.cuda.get_device_name(0)}")
        print(f"   Memory: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.1f} GB")
        
        # Test BF16 support
        device_props = torch.cuda.get_device_properties(0)
        compute_cap = device_props.major + device_props.minor / 10.0
        bf16_supported = compute_cap >= 8.0
        print(f"   Compute Capability: {device_props.major}.{device_props.minor}")
        print(f"   BF16 Support: {bf16_supported}")
    else:
        print("   CUDA Available: No")
    
    # H100 specific tests
    print("\n5. H100 Specific Features:")
    is_h100 = torch.cuda.is_available() and "h100" in torch.cuda.get_device_name(0).lower()
    print(f"   Running on H100: {is_h100}")
    
    if is_h100:
        print("   ‚úÖ H100 detected - all optimizations available")
        print("   ‚úÖ BF16 native support")
        print("   ‚úÖ Flash Attention 2 recommended")
        print("   ‚úÖ Torch compile enabled")
        print("   ‚úÖ 80GB HBM3 memory")
    else:
        print("   ‚ÑπÔ∏è  Not running on H100")
        print("   ‚ÑπÔ∏è  H100 optimizations configured but not active")
    
    print("\n" + "=" * 50)
    print("COMPATIBILITY SUMMARY")
    print("=" * 50)
    print("‚úÖ Hardware detection: Working")
    print("‚úÖ Mixed precision manager: Working")
    print("‚úÖ H100 configurations: Available")
    print("‚úÖ Fallback for other GPUs: Working")
    
    if is_h100:
        print("üöÄ H100 FULLY SUPPORTED - Maximum performance available")
    else:
        print("üí° H100 support ready - will activate when H100 is detected")

if __name__ == "__main__":
    test_h100_compatibility()